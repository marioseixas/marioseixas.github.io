<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>
    
      OpenAI Token Limits — infoBAG
    
  </title>
  <meta name="title" content="infoBAG" />
  <meta name="description" content="can't steer unless already moving" />
  <link rel="canonical" href="https://ib.bsb.br/openai-token-limits/">
  <link rel="alternate" type="application/rss+xml" title="infoBAG" href="https://ib.bsb.br/rss.xml">
  
    <meta itemprop="keywords" content="AI">
    
      <meta property="article:tag" content="AI">
    
  
  <link href="/style.css" rel="stylesheet">
  <link href="/pagefind/pagefind-ui.css" rel="stylesheet">
  <script src="/pagefind/pagefind-ui.js" type="text/javascript"></script>
  <script type="module">
    import PagefindHighlight from '/pagefind/pagefind-highlight.js';
    new PagefindHighlight({ highlightParam: "highlight" });
  </script>
  <script src="/assets/js/search.js" defer></script>
  <script src="/assets/js/prism.js" defer></script>
</head>
<body class="post-content-body">
  <header class="header-container">
    <div id="search" class="search-input-block"></div>
    <nav aria-label="Main navigation" class="header-content">
      <a href="/" aria-label="Home">
        <img src="/assets/Sudden_Death_Rune.gif" alt="Sudden Death Rune" class="favicon">
      </a>
      <a href="https://ib.bsb.br/archive" aria-label="Archive">
        <img src="/favicon.ico" alt="Archive Icon" class="favicon">
      </a>
      <a href="https://ib.bsb.br/tags" aria-label="Tags">
        <img src="/assets/Label.gif" alt="Tags Icon" class="favicon">
      </a>
      <a href="https://ib.bsb.br/events" aria-label="Events">
        <img src="/assets/Paralyse_Rune.gif" alt="Events Icon" class="favicon">
      </a>
    </nav>
  </header>
  <article class="post-wrapper">
    <div class="post-heading">
      <h1 class="post-title">OpenAI Token Limits</h1>
      <div class="post-meta">
        <time datetime="2023-03-10T03:00:00+00:00" class="post-date">
          10 Mar 2023</time><span class="post-updated"> &rightarrowtail; <time datetime="2024-07-13T13:46:16+00:00">13 Jul 2024
          </time>
          </span>
        
        
        <div class="post-info">
          Edit: aberto.
        </div>
        
        
        <div class="post-tags">
          Tags:
          
          <a href="https://ib.bsb.br/tags/#ai" class="tag">
            AI
          </a>
          
          
        </div>
        
        <div class="post-actions">
          <a href="https://github.com/marioseixas/marioseixas.github.io/edit/main/_posts/2023-03-10-openai-token-limits.md" target="_blank" rel="noopener noreferrer" class="btn-primary">
            Improve this page
          </a>&hArr;<a href="https://github.com/marioseixas/marioseixas.github.io/commits/main/_posts/2023-03-10-openai-token-limits.md" target="_blank" rel="noopener noreferrer" class="btn-secondary">
            View revision history
          </a>
        </div>
      </div>
    </div>
    <aside class="aside">
      

      <p>How to Get Around OpenAI GPT-3 Token Limits
Python Developer’s Guide to OpenAI GPT-3 API
UPDATED: The article includes the ChatGPT API option (model=”gpt-3.5-turbo”).</p>

<p>If you are reading this article, you have encountered the token limits of OpenAI’s GPT-3 models. The limits for various models are provided here.</p>

<p>https://platform.openai.com/docs/models/gpt-3
To overcome this limitation, OpenAI offers a great example with the Summarizing Books with Human Feedback (https://openai.com/blog/summarizing-books/) solution as provided below.</p>

<p>The original text is divided into sections, and each section is summarized.
Section summaries are summarized again into higher-level summaries.
The summarizing process continues until a complete summary is achieved.
I faced a similar issue while working on the article “Creating Meeting Minutes using OpenAI GPT-3 API” because most meeting transcripts exceed 4,000 tokens.</p>

<p>This, like everything, has many different ways to be completed. For now, I am providing three options:</p>

<p>NLTK (Natural Language Toolkit). Token count using this option does not match OpenAI tokenizer, but the difference is nominal.
Transformers. Token count using this option matches OpenAI tokenizer.
Tiktoken. Token count using this option matches OpenAI tokenizer and is faster than Transformers.
NLTK
NLTK is a leading platform for building Python programs to work with human language data. This code was developed using Google Colab, but you can use any IDE of your choice. Some codes are specific to Goole Colab, though.</p>

<p>Prerequisites
The following are prerequisites for this tutorial:</p>

<p>Python Package: nltk (Natural Language Toolkit)
Python Package: openai
Install Python Packages
%%writefile requirements.txt
openai
nltk
%pip install -r requirements.txt
Import Python Packages
import platform
import os
import openai
import os</p>

<p>import nltk
nltk.download(‘punkt’)
from nltk.tokenize import word_tokenize</p>

<p>print(‘Python: ‘, platform.python_version())
print(‘re: ‘, re.<strong>version</strong>)
print(‘nltk: ‘, nltk.<strong>version</strong>)</p>

<p>(Optional) Count the Number of Tokens
OpenAI GPT-3 is limited to 4,001 tokens per request, encompassing both the request (i.e., prompt) and response. We will be determining the number of tokens present in the meeting transcript.</p>

<p>def count_tokens(filename):
    with open(filename, ‘r’) as f:
        text = f.read()
    tokens = word_tokenize(text)
    return len(tokens)
filename = “/content/drive/MyDrive/Colab Notebooks/minutes/data/Round_22_Online_Kickoff_Meeting.txt”
token_count = count_tokens(filename)
print(f”Number of tokens: {token_count}”)</p>

<p>(Optional for the second block of code) Break up the Meeting Transcript into chunks of 2,000 tokens with an overlap of 100 tokens
We will be dividing the meeting transcript into segments of 2,000 tokens, with an overlap of 100 tokens to avoid losing any information from the split.</p>

<p>def break_up_file(tokens, chunk_size, overlap_size):
    if len(tokens) &lt;= chunk_size:
        yield tokens
    else:
        chunk = tokens[:chunk_size]
        yield chunk
        yield from break_up_file(tokens[chunk_size-overlap_size:], chunk_size, overlap_size)</p>

<p>def break_up_file_to_chunks(filename, chunk_size=2000, overlap_size=100):
    with open(filename, ‘r’) as f:
        text = f.read()
    tokens = word_tokenize(text)
    return list(break_up_file(tokens, chunk_size, overlap_size))
filename = “/content/drive/MyDrive/Colab Notebooks/minutes/data/Round_22_Online_Kickoff_Meeting.txt”</p>

<p>chunks = break_up_file_to_chunks(filename)
for i, chunk in enumerate(chunks):
    print(f”Chunk {i}: {len(chunk)} tokens”)</p>

<p>Function to Convert the NLTK Tokenized Text to Non-Tokenized Text
We will need to convert the tokenized text from NLTK to non-tokenized text, as the OpenAI GPT-3 API does not handle tokenized text very well, which can result in a higher token count exceeding 2,000.</p>

<p>def convert_to_detokenized_text(tokenized_text):
    prompt_text = “ “.join(tokenized_text)
    prompt_text = prompt_text.replace(“ ‘s”, “‘s”)
    return detokenized_text
Set OpenAI API Key
Set an environment variable called “OPEN_API_KEY” and assign a secret API key from OpenAI (https://beta.openai.com/account/api-keys).</p>

<p>os.environ[“OPENAI_API_KEY”] = ‘your openai api key’
openai.api_key = os.getenv(“OPENAI_API_KEY”)
Summarize the Meeting Transcript one chunk (of 2,000 tokens) at a time
Option 1: model=”text-davinci-003”</p>

<p>filename = “/content/drive/MyDrive/Colab Notebooks/minutes/data/Round_22_Online_Kickoff_Meeting.txt”</p>

<p>prompt_response = []
chunks = break_up_file_to_chunks(filename)</p>

<p>for i, chunk in enumerate(chunks):
    prompt_request = “Summarize this meeting transcript: “ + convert_to_detokenized_text(chunks[i])
    response = openai.Completion.create(
            model=”text-davinci-003”,
            prompt=prompt_request,
            temperature=.5,
            max_tokens=500,
            top_p=1,
            frequency_penalty=0,
            presence_penalty=0
    )</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><section><code>prompt_response.append(response["choices"][0]["text"].strip()) Option 2: model=”gpt-3.5-turbo”
</code></section></div></div>

<p>filename = “/content/drive/MyDrive/Colab Notebooks/minutes/data/Round_22_Online_Kickoff_Meeting.txt”</p>

<p>prompt_response = []
chunks = break_up_file_to_chunks(filename)</p>

<p>for i, chunk in enumerate(chunks):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><section><code>prompt_request = "Summarize this meeting transcript: " + convert_to_prompt_text(chunks[i])

messages = [{"role": "system", "content": "This is text summarization."}]    
messages.append({"role": "user", "content": prompt_request})

response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=messages,
        temperature=.5,
        max_tokens=500,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0
)

prompt_response.append(response["choices"][0]["message"]['content'].strip()) Consolidate the Meeting Transcript Summaries prompt_request = "Consoloidate these meeting summaries: " + str(prompt_response) response = openai.Completion.create(
    model="text-davinci-003",
    prompt=prompt_request,
    temperature=.5,
    max_tokens=1000,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0
) meeting_summary = response["choices"][0]["text"].strip() print(meeting_summary) Transformers Transformers provides APIs and tools to easily download and train state-of-the-art pretrained models. This code was developed using Google Colab, but you can use any IDE of your choice. Some codes are specific to Goole Colab, though.
</code></section></div></div>

<p>Prerequisites
The following are prerequisites for this tutorial:</p>

<p>Python Package: torch
Python Package: transformers
Python Package: openai
Install Python Packages
%%writefile requirements.txt
openai
torch
transformers
%pip install -r requirements.txt
Import Python Packages
import platform
import os
import openai</p>

<p>import torch
import transformers
from transformers import AutoTokenizer</p>

<p>print(‘Python: ‘, platform.python_version())
print(‘re: ‘, re.<strong>version</strong>)
print(‘torch: ‘, torch.<strong>version</strong>)
print(‘transformers: ‘, transformers.<strong>version</strong>)</p>

<p>(Optional) Count the Number of Tokens
OpenAI GPT-3 is limited to 4,001 tokens per request, encompassing both the request (i.e., prompt) and response. We will be determining the number of tokens present in the meeting transcript.</p>

<p>def count_tokens(filename):
    tokenizer = AutoTokenizer.from_pretrained(“gpt2”)
    with open(filename, ‘r’) as f:
        text = f.read()</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><section><code>input_ids = torch.tensor(tokenizer.encode(text)).unsqueeze(0)
num_tokens = input_ids.shape[1]
return num_tokens filename = "/content/drive/MyDrive/Colab Notebooks/minutes/data/Round_22_Online_Kickoff_Meeting.txt" token_count = count_tokens(filename) print(f"Number of tokens: {token_count}")
</code></section></div></div>

<p>(Optional for the second block of code) Break up text into chunks of 2000 tokens with an overlap of 100 tokens
We will be breaking up the text into chunks of 2,000 tokens with an overlapping 100 tokens to ensure any information is not lost from breaking up the text.</p>

<p>def break_up_file_to_chunks(filename, chunk_size=2000, overlap=100):
    tokenizer = AutoTokenizer.from_pretrained(“gpt2”)
    with open(filename, ‘r’) as f:
        text = f.read()</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><section><code>tokens = tokenizer.encode(text)
num_tokens = len(tokens)

chunks = []
for i in range(0, num_tokens, chunk_size - overlap):
    chunk = tokens[i:i + chunk_size]
    chunks.append(chunk)

return chunks filename = "/content/drive/MyDrive/Colab Notebooks/minutes/data/Round_22_Online_Kickoff_Meeting.txt"
</code></section></div></div>

<p>chunks = break_up_file_to_chunks(filename)
for i, chunk in enumerate(chunks):
    print(f”Chunk {i}: {len(chunk)} tokens”)</p>

<p>Set OpenAI API Key
Set an environment variable called “OPEN_API_KEY” and assign a secret API key from OpenAI (https://beta.openai.com/account/api-keys).</p>

<p>os.environ[“OPENAI_API_KEY”] = ‘paste your openai api key here’
openai.api_key = os.getenv(“OPENAI_API_KEY”)
Summarize the text one chunk at a time
Option 1: model=”text-davinci-003”</p>

<p>filename = “/content/drive/MyDrive/Colab Notebooks/minutes/data/Round_22_Online_Kickoff_Meeting.txt”</p>

<p>prompt_response = []</p>

<p>tokenizer = AutoTokenizer.from_pretrained(“gpt2”)</p>

<p>chunks = break_up_file_to_chunks(filename)</p>

<p>for i, chunk in enumerate(chunks):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><section><code>prompt_request = "Summarize this meeting transcript: " + tokenizer.decode(chunks[i])
response = openai.Completion.create(
        model="text-davinci-003",
        prompt=prompt_request,
        temperature=.5,
        max_tokens=500,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0
)

prompt_response.append(response["choices"][0]["text"].strip()) Option 2: model=”gpt-3.5-turbo”
</code></section></div></div>

<p>filename = “/content/drive/MyDrive/Colab Notebooks/minutes/data/Round_22_Online_Kickoff_Meeting.txt”</p>

<p>prompt_response = []</p>

<p>tokenizer = AutoTokenizer.from_pretrained(“gpt2”)</p>

<p>chunks = break_up_file_to_chunks(filename)</p>

<p>for i, chunk in enumerate(chunks):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><section><code>prompt_request = "Summarize this meeting transcript: " + tokenizer.decode(chunks[i])

messages = [{"role": "system", "content": "This is text summarization."}]    
messages.append({"role": "user", "content": prompt_request})

response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=messages,
        temperature=.5,
        max_tokens=500,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0
)

prompt_response.append(response["choices"][0]["message"]['content'].strip()) Consolidate the summaries prompt_request = "Consoloidate these meeting summaries: " + str(prompt_response) response = openai.Completion.create(
    model="text-davinci-003",
    prompt=prompt_request,
    temperature=.5,
    max_tokens=1000,
    top_p=1,
    frequency_penalty=0,
    presence_penalty=0
) Summary of summaries meeting_summary = response["choices"][0]["text"].strip() print(meeting_summary) Tiktoken tiktoken is a fast BPE tokeniser for use with OpenAI’s models. This code was developed using Google Colab, but you can use any IDE of your choice. Some codes are specific to Goole Colab, though.
</code></section></div></div>

<p>Prerequisites
The following are prerequisites for this tutorial:</p>

<p>Python Package: tiktoken
Python Package: openai
Install Python Packages
%%writefile requirements.txt
openai
tiktoken
%pip install -r requirements.txt
Import Python Packages
import platform
import os
import openai
import tiktoken</p>

<p>print(‘Python: ‘, platform.python_version())
(Optional) Count the Number of Tokens
OpenAI GPT-3 is limited to 4,001 tokens per request, encompassing both the request (i.e., prompt) and response. We will be determining the number of tokens present in the meeting transcript.</p>

<p>def count_tokens(filename):
    encoding = tiktoken.get_encoding(“gpt2”)
    with open(filename, ‘r’) as f:
        text = f.read()</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><section><code>input_ids = encoding.encode(text)
num_tokens = len(input_ids)
return num_tokens filename = "/content/drive/MyDrive/Colab Notebooks/minutes/data/Round_22_Online_Kickoff_Meeting.txt"
</code></section></div></div>

<p>num_tokens = count_tokens(filename=filename)
print(“Number of tokens:  “, num_tokens)</p>

<p>(Optional for the second block of code) Break up text into chunks of 2000 tokens with an overlap of 100 tokens
We will be breaking up the text into chunks of 2,000 tokens with an overlapping 100 tokens to ensure any information is not lost from breaking up the text.</p>

<p>def break_up_file_to_chunks(filename, chunk_size=2000, overlap=100):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><section><code>encoding = tiktoken.get_encoding("gpt2")
with open(filename, 'r') as f:
    text = f.read()

tokens = encoding.encode(text)
num_tokens = len(tokens)

chunks = []
for i in range(0, num_tokens, chunk_size - overlap):
    chunk = tokens[i:i + chunk_size]
    chunks.append(chunk)

return chunks filename = "/content/drive/MyDrive/Colab Notebooks/minutes/data/Round_22_Online_Kickoff_Meeting.txt"
</code></section></div></div>

<p>chunks = break_up_file_to_chunks(filename)
for i, chunk in enumerate(chunks):
    print(f”Chunk {i}: {len(chunk)} tokens”)</p>

<p>Set OpenAI API Key
Set an environment variable called “OPEN_API_KEY” and assign a secret API key from OpenAI (https://beta.openai.com/account/api-keys).</p>

<p>os.environ[“OPENAI_API_KEY”] = ‘paste your openai api key here’
openai.api_key = os.getenv(“OPENAI_API_KEY”)
Summarize the text one chunk at a time
Option 1: model=”text-davinci-003”</p>

<p>filename = “/content/drive/MyDrive/Colab Notebooks/minutes/data/Round_22_Online_Kickoff_Meeting.txt”</p>

<p>prompt_response = []</p>

<p>encoding = tiktoken.get_encoding(“gpt2”)
chunks = break_up_file_to_chunks(filename)</p>

<p>for i, chunk in enumerate(chunks):
    prompt_request = “Summarize this meeting transcript: “ + encoding.decode(chunks[i])</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><section><code>response = openai.Completion.create(
        model="text-davinci-003",
        prompt=prompt_request,
        temperature=.5,
        max_tokens=500,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0
)

prompt_response.append(response["choices"][0]["text"].strip()) Option 2: model=”gpt-3.5-turbo”
</code></section></div></div>

<p>filename = “/content/drive/MyDrive/Colab Notebooks/minutes/data/Round_22_Online_Kickoff_Meeting.txt”</p>

<p>prompt_response = []</p>

<p>encoding = tiktoken.get_encoding(“gpt2”)
chunks = break_up_file_to_chunks(filename)</p>

<p>for i, chunk in enumerate(chunks):</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><section><code>prompt_request = "Summarize this meeting transcript: " + encoding.decode(chunks[i])
messages = [{"role": "system", "content": "This is text summarization."}]    
messages.append({"role": "user", "content": prompt_request})

response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=messages,
        temperature=.5,
        max_tokens=500,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0
)

prompt_response.append(response["choices"][0]["message"]['content'].strip()) Consolidate the summaries prompt_request = "Consoloidate these meeting summaries: " + str(prompt_response)
</code></section></div></div>

<p>response = openai.Completion.create(
        model=”text-davinci-003”,
        prompt=prompt_request,
        temperature=.5,
        max_tokens=1000,
        top_p=1,
        frequency_penalty=0,
        presence_penalty=0
    )
Summary of Summaries
meeting_summary = response[“choices”][0][“text”].strip()
print(meeting_summary)
I hope you have enjoyed this tutorial. If you have any questions or comments, please provide them here.</p>

      <main class="content">        
      </main>
      
      <div class="nav-arrow prev">
        <a href="/pypdf/" title="github.com/py-pdf">
          <span aria-hidden="true"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAAAXNSR0IArs4c6QAAAxVJREFUaEPt18FqG1cUBuD/nLkzI42UiVInkFLTCAKhS+9KA+3Kq1L8AAHjhR7AKz+ADV55qUfoqhvThTfdmC4KzTKmYJrELshJW9PIlSeKRpqZe88pUtouihvVRknqMNoMzD1zdPXNrzMM4RJ+6BLuGeWm39RdK6VL6VcIlPEo41HG401loJQupS8mUM7pi7md/6p3UtoHUJzf4vVecZb0e0EQfB7H8b0kSeqqummt/eb1buN83SebNsZ8QkQCYHVpaenu/v7+ra2tLRwdHWF1ddWq6hfM3BCRE2b+FcBHACQAHo1E5v9c+5mIhkR0e3z0VQ8LotuqWlXVw/GRmT8QkdMK89McuAOAAfwoIu8z85yIHBPRKRGN1/IAeDx0rul53hivo6psrf2ejDGfEtFOu92u7O3tBWtra2i322g0GhgOh9je3ka325UPax7fCR0iw+lp4aLxr70aeOmgkChTRY3JGSb73EroERD73uB54WpOgdhwZkXNQNQLiVDzOU1yFymAhu+lqZVopIqISQKmPLFS+atHv3A1q8AVQ/lXP/VHRPQx+b6frKysxMvLy1hfX0ez2USv18PCwgLiOEaSJKhWq/jhwQN8dnIfVOS4dsWHqCJ5YREGjBuNEMfd0eTc9WshBqnFYGhRqxrUIoNuLwMT4eb1Cp6dZshywdW6mZzr9QsEPuPmXIhfno3gnGKuEWCUC14MLKoVb1L72+8Zvj3JOl/udZt/S7darUqn0wlarRZ2d3cxPz+PJ8dPsPP1Doq8kIbL+cawjxBIM8MRFAidpBlRpIbhFc55gM2NF5IIQtVBRlRTZgTWZQ4wzvc8spO1NPM4Gr9Wh1bSDIjU98C5E0PIC8OVSX+Rlz08hl+4/LsML6X/menFxcW7BwcHtzY2NvDw8CE21zf/NdMAHskFMs3MT/EfMg3gsTsr02f8byfTo16v3+v3+//f6fGKgXNp5vT5huZbqH4nH+NvwXH6V5bS041mU1FKz8ZxepdSerrRbCpK6dk4Tu9SSk83mk1FKT0bx+ldSunpRrOpuJTSfwBcuImxpyA7XgAAAABJRU5ErkJggg==" alt="Previous" style="max-width: 45px; height: auto;"></span>
        </a>
      </div>
      
      
      <div class="nav-arrow next">
        <a href="/maio-23/" title="Maio 2023">
          <span aria-hidden="true"><img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAC0AAAAtCAYAAAA6GuKaAAAAAXNSR0IArs4c6QAAAwdJREFUaEPtl01rG1cUht9z7p070ijKWLVS3I9YULtNMBRn2UIXxZtuvCjedNWfoKVXBvsHeOUfoIUx5AeY0pVXXSR052UxBJSi0kDiRplII83HvadI9CMtOFjCmBjubC7cc8+Zc555eYdLuIEP3cCe4Zu+rq/mSXvSbyHg5eHl4eVxXRrwpD3p+Qh4n56P2+xZnvTszC7MCAAUF0XfOdJa62+IaCeO40GSJA/zPP8RwB9vDjBtWmv9JRE5ImpZawdVpbo58CkAIyJnIrLAzEvOuXNm/h3AfQDOAGdj5z5m5gXn3G9ENCKilckaiDwpiFZEpCoiTyYrM3/knOtXmHs58BkABvCLc+4DZl6cxIjoh4ODA728vIzt7W2sra09PT4+fgTgQES4LMvHZIy5LyI/f/dJvfK6FKMJqAdqmBS2ZgWINY9zJyZ1whUiRJrTfmGjybSxUemwcFEmghqT1UxlUrpQEXD7jRq3NWelEz10okIi1AJOX+U2EgALgUrT0kVnmcKvQ+uazSZvbW2hWq2i3++j3W5jf38f6+vrebvdHovI5pT09+vN7teLYev990K8GpQYjS1u1TQqhnHez6EU4cM7FTw7z5AXDo16ACcyPRsaxp2FEM9ejKd7zUaIYVpiOCpRq2rUIo0XLzMwEZaaFTzvZ8hyh/iWnu69fF1AAoOfFr/A5w8eYDQaIY5jJEmC09NTNBoNdLtd7O3t4ejoCIeHh8k/pL8KUSkCZcg6hCLDjLk2ufYGpRuXAuOMYiosQiDNNEcQILQuzYgi0QxVWKuAMtcqJPdXDaKaMMOUNrOAtoFSVE5jaaY4mtQPS5dmQPS8WkdfGReYgDe/3cTdpbvo9XrY2NhAp9NBq9XKO53Ov6T/r2mlVBeX0DSAMzeHppm5h7doemdvR99buYfd3V2srq4+PTk5+a+mr9CmrqTU3+5Rr9cHg8HgYve4krddbZGb5dOXmf2d+7n4pi9D4LrOeHl40v42fl0a8KQ96fkIeJ+ej9vsWZ707Mzmy/Ck5+M2e9afhmuAz1BhwewAAAAASUVORK5CYII=" alt="Next" style="max-width: 45px; height: auto;"></span>
        </a>
      </div>
      
    </aside>
    
    <div class="comment-box">
      <a href="#" aria-label="Back to top"><img src="/assets/gold.ico" alt="Back to top"></a>&#10230;<a href="github.com/Ighina/DeepTiling" title="page.comment">github.com/Ighina/DeepTiling</a>
    </div>
    
  </article>
</body>
</html>
